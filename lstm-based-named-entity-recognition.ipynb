{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/abhijitbhandari/lstm-based-named-entity-recognition?scriptVersionId=143374564\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"## Importing Required Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install chardet","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import chardet\n# with open(\"./ner_dataset.csv\", \"rb\") as f:\n#     byte_data = f.read(100000) #Read the first 1024 bytes\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# result = chardet.detect(byte_data)\n# encoding = result[\"encoding\"]\n##encoding is 'Windows-1252'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reading the csv file","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/entity-annotated-corpus/ner_dataset.csv\", encoding= \"Windows-1252\")\ndf.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"Sentence #\"].unique()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (4,3))\ndf.isnull().sum().plot(kind = \"bar\")\nplt.xlabel(\"Columns\")\nplt.ylabel(\"Total Null Values\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.fillna(method = \"ffill\", axis = 0, inplace = True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"Sentence #\"].unique()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Understanding the Data","metadata":{}},{"cell_type":"code","source":"df.head(10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"From the above dataframe, we can infer that the first column refers to the sentence where the word in Word column is present.\n\n**Essential info about entities:**\n\ngeo = Geographical Entity\norg = Organization\nper = Person\ngpe = Geopolitical Entity\ntim = Time indicator\nart = Artifact\neve = Event\nnat = Natural Phenomenon\nTotal Words Count = 1354149\nTarget Data Column: \"tag\"rget Data C\n\nSource: https://www.kaggle.com/datasets/abhinavwalia95/entity-annotated-corpus?select=ner_dataset.csvlumn: \"tag\"","metadata":{}},{"cell_type":"code","source":"## Removing any row with number\ndf = df[~df[\"Word\"].str.contains(r\"\\d+(\\.\\d+)?\")]\ndf.reset_index(drop = True, inplace = True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (7,5))\nax = df.nunique().plot(kind = \"bar\")\nplt.xlabel(\"Column Name\")\nplt.ylabel(\"Total Number of Unique Items\")\nplt.xticks(rotation = 45)\nplt.tight_layout()\n\nfor i, v in enumerate(df.nunique()):\n    ax.text(i, v+0.2, str(v), ha = \"center\", va = \"bottom\")\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, there are 42 parts of speech and 17 tags","metadata":{}},{"cell_type":"code","source":"## Understanding the distribution of tags\ndf.groupby([\"Tag\"])[\"Word\"].count().plot(kind = \"bar\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The tag classes are highly imbalanced","metadata":{}},{"cell_type":"markdown","source":"## Data Wrangling","metadata":{}},{"cell_type":"code","source":"words = list(set(df[\"Word\"].values))\nwords[:5]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"words = [x.lower() for x in words]\nwords[:5]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Append eos (end of sentence)\nwords.append(\"eos\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_size = len(words)\nvocab_size","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tags = list(set(df[\"Tag\"].values))\ntags = [t.lower() for t in tags]\ntags[:5]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_tags = len(tags)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Now, we try to group the data as (word, pos, tag) for every sentence.**","metadata":{}},{"cell_type":"code","source":"df.columns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class sentenceinfo(object):\n    \"\"\"\n    This is a function used to arrange arrange the data as [[(word, pos, tag)],.....] for every sentence\n    present in the dataframe\n    \"\"\"\n    def __init__(self, data:pd.DataFrame):\n        assert all(col for col in data.columns if col in [\"Sentence #\", \"Word\", \"POS\", \"Tag\"]), \"Check the column or change the column names as Sentence #, Word, POS, Tag\"\n        self.n_sent = 1\n        self.data = data\n        agg_func = lambda s: [(w.lower(),p.lower(),t.lower()) for w,p,t in zip(s[\"Word\"].values.tolist(),\n                                                      s[\"POS\"].values.tolist(),\n                                                      s[\"Tag\"].values.tolist())]\n        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n        self.sentences = [s for s in self.grouped]\n\n    def get_next(sentence):\n        try:\n            s = self.sentences[self.n_sent]\n            self.n_sent+=1\n            return s\n        except IndexError:\n            return None\n            \n\n\n        ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"getter = sentenceinfo(df)\nsentences = getter.sentences","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Define Mapping B/w Sentences and Tags","metadata":{}},{"cell_type":"code","source":"word2idx = {w:i for i,w in enumerate(words)}\ntag2idx = {t:i for i,t in enumerate(tags)}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train - Test Data Preparation","metadata":{}},{"cell_type":"markdown","source":"In order to train the model, we need feed sentences of equal lengths. So, figuring out the maximum length of sentences","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (5,3))\nplt.hist([len(s) for s in sentences], bins = 50)\nplt.xlabel(\"Length of Sentence\")\nplt.ylabel(\"Frequency\")\nplt.tight_layout()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, we can see that the most of the sentences are if length 20-22. And the sentence with maximum length has about 60 words.\nSo, taking it as 60.","metadata":{}},{"cell_type":"code","source":"## Padding\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nmax_len = 60\n\nX = [[word2idx[w[0]] for w in s] for s in sentences]\ny = [[tag2idx[t[2]] for t in s] for s in sentences]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = pad_sequences(X,maxlen=max_len, padding=\"post\",value = 0)\ny = pad_sequences(y, maxlen = max_len, padding= \"post\", value = tag2idx[\"o\"])\nprint(X.shape)\nprint(y.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Splitting the data for training and testing\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train,y_test = train_test_split(X,y,test_size = 0.2, random_state = 42)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y[0][:10]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Generating the Embedding Matrix Using Glove Vectors for the words in the corpus","metadata":{}},{"cell_type":"code","source":"embedding_dim = 100\nglove_embedded_words = {}\nembedding_matrix = np.zeros(shape = (vocab_size, embedding_dim))\n\nwith open(\"/kaggle/input/embedding-vector-glove100d/glove.6B.100d_set.txt\",encoding=\"utf-8\") as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        vector = np.array(values[1:],dtype = \"float32\")\n        glove_embedded_words[word] = vector","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for word, idx in word2idx.items():\n    if word!=\"eos\":\n        embedding_vector = glove_embedded_words.get(word, np.zeros(embedding_dim))\n    else:\n        embedding_vector = np.full(embedding_dim, -1)\n    embedding_matrix[idx] = embedding_vector","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Development","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Bidirectional, Dense, Embedding, TimeDistributed, SpatialDropout1D, Dropout, Input","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(input_dim = vocab_size,\n                   output_dim = embedding_dim,\n                   input_length = max_len,\n                   weights = [embedding_matrix],\n                   trainable = False))\n\nmodel.add(SpatialDropout1D(0.15))\nmodel.add(Bidirectional(LSTM(units = 150, return_sequences = True, recurrent_dropout = 0.1)))\nmodel.add(TimeDistributed(Dense(num_tags, activation = \"softmax\")))\nmodel.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"opt = tf.keras.optimizers.Adam(clipvalue = 0.5, learning_rate= 0.001)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer=opt,loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"callbacks = [tf.keras.callbacks.ReduceLROnPlateau(patience = 3, verbose = 1,min_lr = 1e-4),\n            tf.keras.callbacks.EarlyStopping(patience = 4, verbose = 1, restore_best_weights = True)]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 32\nepochs = 20\nhistory = model.fit(x = X_train, y = y_train,batch_size = batch_size,\n                   epochs = epochs,callbacks=callbacks,validation_data = (X_test, y_test))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n                                        \n# # Access training history from the 'history' object\ntraining_loss = history.history['loss']\nvalidation_loss = history.history['val_loss']\ntraining_accuracy = history.history['accuracy']  \nvalidation_accuracy = history.history['val_accuracy'] \n\n# # Create an array representing the number of epochs\nepochs = range(1, len(training_loss) + 1)\n                                        \n# # Plot training and validation loss\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.plot(epochs, training_loss, 'b', label='Training Loss')\nplt.plot(epochs, validation_loss, 'r', label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n                                        \n# # Plot training and validation accuracy\nplt.subplot(1, 2, 2)\nplt.plot(epochs, training_accuracy, 'b', label='Training Accuracy')\nplt.plot(epochs, validation_accuracy, 'r', label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.tight_layout()\nplt.show()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_predicted = model.predict(X_test, batch_size=32)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_predicted = np.argmax(y_predicted, axis = -1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test[3]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_predicted[3]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Comparing Predicted with True with metrics\ny_test_flat = y_test.flatten()\ny_predicted_flat = y_predicted.flatten()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nreport = classification_report(y_test_flat, y_predicted_flat)\nprint(report)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"{'b-gpe': 0,'b-tim': 1,'b-org': 2,'b-per': 3,'o': 4,'i-tim': 5,'b-nat': 6,'i-nat': 7,'i-art': 8,'b-geo': 9, 'i-geo': 10,\n 'i-org': 11,'b-art': 12,'i-eve': 13,'i-gpe': 14,'b-eve': 15,'i-per': 16}\n\n\n1) Here, we can see that the model has good precision, recall and accuracy of 1, for class  which is \"O\"\n\n2) For classes 5,6,7,12, 13 and 15, the model has very poor performance. This is due to class imbalance in the dataset available.","metadata":{}},{"cell_type":"code","source":"model.save(filepath = \"./ner.h5\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Loading the Model","metadata":{}},{"cell_type":"code","source":"from keras.models import load_model\nmodel_ner = load_model(\"./ner.h5\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Testing for a random sentence\n\nsent = \"A handsome man named Ram lived in the house with his beautiful wife Sita in a country called India\"\nsent = sent.lower()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_words = sent.split()\nrandom_words.append(\"eos\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"indexed = [[word2idx[i] for i in random_words]]\nX_random = pad_sequences(indexed, maxlen=60, padding = \"post\", value = 0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_random = np.argmax(model_ner.predict(X_random), axis = -1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tags_random = []\nfor i in y_random[0]:\n    for key,value in tag2idx.items():\n        if i==value:\n            tags_random.append(key)\n\ntags_random = tags_random[:len(random_words)-1]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_df = pd.DataFrame({\"Words\": random_words[:-1], \"Tags\": tags_random})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}